{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d834537a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:46:21.936870Z",
     "iopub.status.busy": "2025-12-13T17:46:21.936279Z",
     "iopub.status.idle": "2025-12-13T17:46:31.176378Z",
     "shell.execute_reply": "2025-12-13T17:46:31.175753Z"
    },
    "papermill": {
     "duration": 9.246015,
     "end_time": "2025-12-13T17:46:31.177683",
     "exception": false,
     "start_time": "2025-12-13T17:46:21.931668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e7df5dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:46:31.185308Z",
     "iopub.status.busy": "2025-12-13T17:46:31.184646Z",
     "iopub.status.idle": "2025-12-13T17:46:31.261880Z",
     "shell.execute_reply": "2025-12-13T17:46:31.261030Z"
    },
    "papermill": {
     "duration": 0.082149,
     "end_time": "2025-12-13T17:46:31.263099",
     "exception": false,
     "start_time": "2025-12-13T17:46:31.180950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "# UPDATE THIS PATH based on your Kaggle Data tab\n",
    "DATA_PATH = \"/kaggle/input/amlfif/Dataset\" \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 50 \n",
    "NUM_CLASSES = 200\n",
    "IMAGE_SIZE = 224\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "281502ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:46:31.270773Z",
     "iopub.status.busy": "2025-12-13T17:46:31.270571Z",
     "iopub.status.idle": "2025-12-13T17:46:31.279023Z",
     "shell.execute_reply": "2025-12-13T17:46:31.278431Z"
    },
    "papermill": {
     "duration": 0.013682,
     "end_time": "2025-12-13T17:46:31.280153",
     "exception": false,
     "start_time": "2025-12-13T17:46:31.266471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ==========================================\n",
    "# # 2. DATASET CLASS\n",
    "# # ==========================================\n",
    "# class BirdDataset(Dataset):\n",
    "#     def __init__(self, csv_file, root_dir, transform=None):\n",
    "#         self.data = pd.read_csv(csv_file)\n",
    "#         self.root_dir = root_dir\n",
    "#         self.transform = transform\n",
    "        \n",
    "#         # Adjust label to be 0-199 (PyTorch starts at 0)\n",
    "#         # Only do this if 'label' column exists (Training data)\n",
    "#         if 'label' in self.data.columns:\n",
    "#             self.data['label'] = self.data['label'] - 1\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # Handle paths\n",
    "#         img_path = self.data.iloc[idx, 0] if 'label' in self.data.columns else self.data.iloc[idx, 1]\n",
    "        \n",
    "#         if img_path.startswith(\"/\"):\n",
    "#             img_path = img_path[1:]\n",
    "            \n",
    "#         full_path = os.path.join(self.root_dir, img_path)\n",
    "        \n",
    "#         try:\n",
    "#             image = Image.open(full_path).convert(\"RGB\")\n",
    "#         except FileNotFoundError:\n",
    "#             image = Image.new('RGB', (IMAGE_SIZE, IMAGE_SIZE))\n",
    "            \n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "            \n",
    "#         # Return image + label (if training) or image + id (if testing)\n",
    "#         if 'label' in self.data.columns:\n",
    "#             label = self.data.iloc[idx, 1]\n",
    "#             return image, torch.tensor(label, dtype=torch.long)\n",
    "#         else:\n",
    "#             image_id = self.data.iloc[idx, 0]\n",
    "#             return image, image_id\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. DATASET CLASS (Fixed)\n",
    "# ==========================================\n",
    "# class BirdDataset(Dataset):\n",
    "#     def __init__(self, csv_file, root_dir, transform=None):\n",
    "#         self.data = pd.read_csv(csv_file)\n",
    "#         self.root_dir = root_dir\n",
    "#         self.transform = transform\n",
    "        \n",
    "#         # Adjust label to be 0-199 (PyTorch starts at 0)\n",
    "#         # Only do this if 'label' column exists (Training data)\n",
    "#         if 'label' in self.data.columns:\n",
    "#             self.data['label'] = self.data['label'] - 1\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # Get the row\n",
    "#         row = self.data.iloc[idx]\n",
    "        \n",
    "#         # --- SMART DETECTION START ---\n",
    "#         # We need to find which column has the image path (string) and which has the label/id (int)\n",
    "#         # We check the first column (index 0). Is it a string ending in .jpg?\n",
    "#         val0 = row[0]\n",
    "#         val1 = row[1]\n",
    "        \n",
    "#         if isinstance(val0, str) and (val0.endswith('.jpg') or '/' in val0):\n",
    "#             img_path = val0\n",
    "#             label_or_id = val1\n",
    "#         else:\n",
    "#             # If col 0 isn't the path, then col 1 must be the path\n",
    "#             img_path = val1\n",
    "#             label_or_id = val0\n",
    "#         # --- SMART DETECTION END ---\n",
    "        \n",
    "#         # Handle leading slashes if present\n",
    "#         if str(img_path).startswith(\"/\"):\n",
    "#             img_path = img_path[1:]\n",
    "            \n",
    "#         full_path = os.path.join(self.root_dir, img_path)\n",
    "        \n",
    "#         try:\n",
    "#             image = Image.open(full_path).convert(\"RGB\")\n",
    "#         except FileNotFoundError:\n",
    "#             # Fallback for missing images to prevent crash\n",
    "#             image = Image.new('RGB', (224, 224))\n",
    "            \n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "            \n",
    "#         # Return proper pair based on Training vs Testing\n",
    "#         if 'label' in self.data.columns:\n",
    "#             # For Training: Return (Image, Label)\n",
    "#             # Ensure label is a tensor\n",
    "#             return image, torch.tensor(label_or_id, dtype=torch.long)\n",
    "#         else:\n",
    "#             # For Testing: Return (Image, ID)\n",
    "#             return image, label_or_id\n",
    "\n",
    "# ==========================================\n",
    "# 2. DATASET CLASS (Fixed & Clean)\n",
    "# ==========================================\n",
    "class BirdDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Adjust label to be 0-199 (PyTorch starts at 0)\n",
    "        if 'label' in self.data.columns:\n",
    "            self.data['label'] = self.data['label'] - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the row by position\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # --- SMART DETECTION START ---\n",
    "        # We use .iloc[] to avoid the FutureWarning\n",
    "        val0 = row.iloc[0]\n",
    "        val1 = row.iloc[1]\n",
    "        \n",
    "        # Check which one is the image path (string ending in .jpg)\n",
    "        if isinstance(val0, str) and (val0.endswith('.jpg') or '/' in val0):\n",
    "            img_path = val0\n",
    "            label_or_id = val1\n",
    "        else:\n",
    "            img_path = val1\n",
    "            label_or_id = val0\n",
    "        # --- SMART DETECTION END ---\n",
    "        \n",
    "        # Handle leading slashes\n",
    "        if str(img_path).startswith(\"/\"):\n",
    "            img_path = img_path[1:]\n",
    "            \n",
    "        full_path = os.path.join(self.root_dir, img_path)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(full_path).convert(\"RGB\")\n",
    "        except FileNotFoundError:\n",
    "            image = Image.new('RGB', (224, 224))\n",
    "            \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # Return proper pair\n",
    "        if 'label' in self.data.columns:\n",
    "            # Training: Return (Image, Label)\n",
    "            return image, torch.tensor(label_or_id, dtype=torch.long)\n",
    "        else:\n",
    "            # Testing: Return (Image, ID)\n",
    "            return image, label_or_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1128d4a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:46:31.286917Z",
     "iopub.status.busy": "2025-12-13T17:46:31.286468Z",
     "iopub.status.idle": "2025-12-13T17:46:31.291447Z",
     "shell.execute_reply": "2025-12-13T17:46:31.290922Z"
    },
    "papermill": {
     "duration": 0.009406,
     "end_time": "2025-12-13T17:46:31.292430",
     "exception": false,
     "start_time": "2025-12-13T17:46:31.283024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. TRANSFORMS (Data Augmentation)\n",
    "# ==========================================\n",
    "# Innovation: Adding augmentation helps the model generalize better\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Flip left/right\n",
    "    transforms.RandomRotation(degrees=15),   # Rotate slightly\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1), # vary lighting\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445dcac8",
   "metadata": {
    "papermill": {
     "duration": 0.002911,
     "end_time": "2025-12-13T17:46:31.298333",
     "exception": false,
     "start_time": "2025-12-13T17:46:31.295422",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf85faa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:46:31.304771Z",
     "iopub.status.busy": "2025-12-13T17:46:31.304566Z",
     "iopub.status.idle": "2025-12-13T17:46:31.311485Z",
     "shell.execute_reply": "2025-12-13T17:46:31.310867Z"
    },
    "papermill": {
     "duration": 0.011415,
     "end_time": "2025-12-13T17:46:31.312535",
     "exception": false,
     "start_time": "2025-12-13T17:46:31.301120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. CUSTOM MODEL (Built from Scratch)\n",
    "# ==========================================\n",
    "class SimpleBirdCNN(nn.Module):\n",
    "    def __init__(self, num_classes=200):\n",
    "        super(SimpleBirdCNN, self).__init__()\n",
    "        \n",
    "        # Block 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2) # Reduces size / 2\n",
    "        \n",
    "        # Block 2\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Block 3\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Block 4\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5) # Prevents overfitting\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        # After 4 pools (224 -> 112 -> 56 -> 28 -> 14), image is 14x14\n",
    "        self.fc1 = nn.Linear(256 * 14 * 14, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through convolutions\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(self.relu(self.bn4(self.conv4(x))))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Classifier\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367e1869",
   "metadata": {
    "papermill": {
     "duration": 0.002836,
     "end_time": "2025-12-13T17:46:31.318203",
     "exception": false,
     "start_time": "2025-12-13T17:46:31.315367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Improvised Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd4e887f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:46:31.324778Z",
     "iopub.status.busy": "2025-12-13T17:46:31.324585Z",
     "iopub.status.idle": "2025-12-13T17:46:31.336598Z",
     "shell.execute_reply": "2025-12-13T17:46:31.335990Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.01669,
     "end_time": "2025-12-13T17:46:31.337636",
     "exception": false,
     "start_time": "2025-12-13T17:46:31.320946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# IMPROVED MODEL: Mini-ResNet\n",
    "# ==========================================\n",
    "\n",
    "# 1. The Building Block (The Innovation)\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        # First convolution\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Second convolution\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # The \"Skip Connection\" logic\n",
    "        # If the input size changes (due to stride), we need to resize the shortcut too\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x # Save the original input (the \"jump\")\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        out += self.shortcut(identity) # ADD the original input back here\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# 2. The Full Network\n",
    "class ResNetFromScratch(nn.Module):\n",
    "    def __init__(self, num_classes=200):\n",
    "        super(ResNetFromScratch, self).__init__()\n",
    "        \n",
    "        # Initial processing (Entry point)\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Stack the Residual Blocks (Deeper Architecture)\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1)  # 2 blocks\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2) # 2 blocks\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2) # 2 blocks\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2) # 2 blocks\n",
    "        \n",
    "        # Classifier\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, out_channels, blocks, stride):\n",
    "        layers = []\n",
    "        # First block handles the stride (downsampling)\n",
    "        layers.append(ResidualBlock(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels\n",
    "        \n",
    "        # Remaining blocks just process features\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72477f81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:46:31.344521Z",
     "iopub.status.busy": "2025-12-13T17:46:31.344338Z",
     "iopub.status.idle": "2025-12-13T17:46:31.352626Z",
     "shell.execute_reply": "2025-12-13T17:46:31.352028Z"
    },
    "papermill": {
     "duration": 0.013089,
     "end_time": "2025-12-13T17:46:31.353675",
     "exception": false,
     "start_time": "2025-12-13T17:46:31.340586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# IMPROVED MODEL: ResNet with Dropout\n",
    "# ==========================================\n",
    "class ResNetFromScratch(nn.Module):\n",
    "    def __init__(self, num_classes=200):\n",
    "        super(ResNetFromScratch, self).__init__()\n",
    "        \n",
    "        # Initial processing (Entry point)\n",
    "        self.in_channels = 64\n",
    "        # Standard ResNet Start\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Stack the Residual Blocks\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
    "        \n",
    "        # Classifier\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # --- INNOVATION: Dropout ---\n",
    "        # Dropping 50% of neurons prevents overfitting on small datasets\n",
    "        self.dropout = nn.Dropout(p=0.5) \n",
    "        \n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "        # Initialize weights (Helps training start better)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, out_channels, blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        # Apply Dropout before the final classification\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba0d736",
   "metadata": {
    "papermill": {
     "duration": 0.002942,
     "end_time": "2025-12-13T17:46:31.359479",
     "exception": false,
     "start_time": "2025-12-13T17:46:31.356537",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b620462",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:46:31.366430Z",
     "iopub.status.busy": "2025-12-13T17:46:31.365897Z",
     "iopub.status.idle": "2025-12-13T17:46:31.400260Z",
     "shell.execute_reply": "2025-12-13T17:46:31.399739Z"
    },
    "papermill": {
     "duration": 0.038905,
     "end_time": "2025-12-13T17:46:31.401270",
     "exception": false,
     "start_time": "2025-12-13T17:46:31.362365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5. SETUP LOADERS & MODEL\n",
    "# ==========================================\n",
    "# Load Data\n",
    "full_dataset = BirdDataset(\n",
    "    csv_file=f'{DATA_PATH}/train_images.csv', \n",
    "    root_dir=f'{DATA_PATH}',\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "# Split 80/20\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# IMPORTANT: Validation set should NOT use augmentation (just resize)\n",
    "val_dataset.dataset.transform = val_transforms \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bfd994",
   "metadata": {
    "papermill": {
     "duration": 0.003108,
     "end_time": "2025-12-13T17:46:31.407201",
     "exception": false,
     "start_time": "2025-12-13T17:46:31.404093",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efe93ccc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:46:31.413766Z",
     "iopub.status.busy": "2025-12-13T17:46:31.413595Z",
     "iopub.status.idle": "2025-12-13T17:58:08.732445Z",
     "shell.execute_reply": "2025-12-13T17:58:08.731491Z"
    },
    "papermill": {
     "duration": 697.323614,
     "end_time": "2025-12-13T17:58:08.733856",
     "exception": false,
     "start_time": "2025-12-13T17:46:31.410242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture Created from Scratch!\n",
      "Epoch 1/50\n",
      "Train Loss: 7.0816\n",
      "Val Acc: 0.0089\n",
      "Epoch 2/50\n",
      "Train Loss: 5.2926\n",
      "Val Acc: 0.0089\n",
      "Epoch 3/50\n",
      "Train Loss: 5.2868\n",
      "Val Acc: 0.0102\n",
      "Epoch 4/50\n",
      "Train Loss: 5.2859\n",
      "Val Acc: 0.0064\n",
      "Epoch 5/50\n",
      "Train Loss: 5.2802\n",
      "Val Acc: 0.0064\n",
      "Epoch 6/50\n",
      "Train Loss: 5.2737\n",
      "Val Acc: 0.0064\n",
      "Epoch 7/50\n",
      "Train Loss: 5.2708\n",
      "Val Acc: 0.0064\n",
      "Epoch 8/50\n",
      "Train Loss: 5.2646\n",
      "Val Acc: 0.0089\n",
      "Epoch 9/50\n",
      "Train Loss: 5.2670\n",
      "Val Acc: 0.0064\n",
      "Epoch 10/50\n",
      "Train Loss: 5.2590\n",
      "Val Acc: 0.0064\n",
      "Epoch 11/50\n",
      "Train Loss: 5.2573\n",
      "Val Acc: 0.0064\n",
      "Epoch 12/50\n",
      "Train Loss: 5.2529\n",
      "Val Acc: 0.0064\n",
      "Epoch 13/50\n",
      "Train Loss: 5.2485\n",
      "Val Acc: 0.0064\n",
      "Epoch 14/50\n",
      "Train Loss: 5.2456\n",
      "Val Acc: 0.0064\n",
      "Epoch 15/50\n",
      "Train Loss: 5.2454\n",
      "Val Acc: 0.0064\n",
      "Epoch 16/50\n",
      "Train Loss: 5.2423\n",
      "Val Acc: 0.0064\n",
      "Epoch 17/50\n",
      "Train Loss: 5.2390\n",
      "Val Acc: 0.0064\n",
      "Epoch 18/50\n",
      "Train Loss: 5.2375\n",
      "Val Acc: 0.0064\n",
      "Epoch 19/50\n",
      "Train Loss: 5.2391\n",
      "Val Acc: 0.0064\n",
      "Epoch 20/50\n",
      "Train Loss: 5.2338\n",
      "Val Acc: 0.0064\n",
      "Epoch 21/50\n",
      "Train Loss: 5.2321\n",
      "Val Acc: 0.0064\n",
      "Epoch 22/50\n",
      "Train Loss: 5.2303\n",
      "Val Acc: 0.0064\n",
      "Epoch 23/50\n",
      "Train Loss: 5.2287\n",
      "Val Acc: 0.0064\n",
      "Epoch 24/50\n",
      "Train Loss: 5.2271\n",
      "Val Acc: 0.0064\n",
      "Epoch 25/50\n",
      "Train Loss: 5.2257\n",
      "Val Acc: 0.0064\n",
      "Epoch 26/50\n",
      "Train Loss: 5.2242\n",
      "Val Acc: 0.0064\n",
      "Epoch 27/50\n",
      "Train Loss: 5.2230\n",
      "Val Acc: 0.0064\n",
      "Epoch 28/50\n",
      "Train Loss: 5.2217\n",
      "Val Acc: 0.0064\n",
      "Epoch 29/50\n",
      "Train Loss: 5.2205\n",
      "Val Acc: 0.0064\n",
      "Epoch 30/50\n",
      "Train Loss: 5.2194\n",
      "Val Acc: 0.0064\n",
      "Epoch 31/50\n",
      "Train Loss: 5.2183\n",
      "Val Acc: 0.0064\n",
      "Epoch 32/50\n",
      "Train Loss: 5.2172\n",
      "Val Acc: 0.0064\n",
      "Epoch 33/50\n",
      "Train Loss: 5.2163\n",
      "Val Acc: 0.0064\n",
      "Epoch 34/50\n",
      "Train Loss: 5.2153\n",
      "Val Acc: 0.0064\n",
      "Epoch 35/50\n",
      "Train Loss: 5.2145\n",
      "Val Acc: 0.0064\n",
      "Epoch 36/50\n",
      "Train Loss: 5.2137\n",
      "Val Acc: 0.0064\n",
      "Epoch 37/50\n",
      "Train Loss: 5.2128\n",
      "Val Acc: 0.0064\n",
      "Epoch 38/50\n",
      "Train Loss: 5.2121\n",
      "Val Acc: 0.0064\n",
      "Epoch 39/50\n",
      "Train Loss: 5.2114\n",
      "Val Acc: 0.0064\n",
      "Epoch 40/50\n",
      "Train Loss: 5.2107\n",
      "Val Acc: 0.0064\n",
      "Epoch 41/50\n",
      "Train Loss: 5.2101\n",
      "Val Acc: 0.0064\n",
      "Epoch 42/50\n",
      "Train Loss: 5.2094\n",
      "Val Acc: 0.0064\n",
      "Epoch 43/50\n",
      "Train Loss: 5.2089\n",
      "Val Acc: 0.0064\n",
      "Epoch 44/50\n",
      "Train Loss: 5.2084\n",
      "Val Acc: 0.0051\n",
      "Epoch 45/50\n",
      "Train Loss: 5.2078\n",
      "Val Acc: 0.0051\n",
      "Epoch 46/50\n",
      "Train Loss: 5.2074\n",
      "Val Acc: 0.0064\n",
      "Epoch 47/50\n",
      "Train Loss: 5.2069\n",
      "Val Acc: 0.0051\n",
      "Epoch 48/50\n",
      "Train Loss: 5.2065\n",
      "Val Acc: 0.0064\n",
      "Epoch 49/50\n",
      "Train Loss: 5.2060\n",
      "Val Acc: 0.0051\n",
      "Epoch 50/50\n",
      "Train Loss: 5.2056\n",
      "Val Acc: 0.0051\n",
      "Best Validation Accuracy: 0.0102\n"
     ]
    }
   ],
   "source": [
    "# Init CNN Model\n",
    "model = SimpleBirdCNN(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Model Architecture Created from Scratch!\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. TRAINING LOOP\n",
    "# ==========================================\n",
    "best_acc = 0.0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}')\n",
    "    \n",
    "    # Train\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Train Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    # Validate\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    val_acc = correct / total\n",
    "    print(f\"Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        torch.save(model.state_dict(), 'best_custom_model.pth')\n",
    "\n",
    "print(f\"Best Validation Accuracy: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53167e7c",
   "metadata": {
    "papermill": {
     "duration": 0.006835,
     "end_time": "2025-12-13T17:58:08.747737",
     "exception": false,
     "start_time": "2025-12-13T17:58:08.740902",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Restnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ee14fa9",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-12-13T17:58:08.762273Z",
     "iopub.status.busy": "2025-12-13T17:58:08.761655Z",
     "iopub.status.idle": "2025-12-13T18:09:47.164309Z",
     "shell.execute_reply": "2025-12-13T18:09:47.163430Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "papermill": {
     "duration": 698.4214,
     "end_time": "2025-12-13T18:09:47.175618",
     "exception": false,
     "start_time": "2025-12-13T17:58:08.754218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Custom ResNet...\n",
      "Starting training on cuda for 50 epochs...\n",
      "\n",
      "Epoch 1/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.4420\n",
      "Val Loss:   5.4547\n",
      "Val Acc:    0.0191\n",
      "Current LR: 0.001\n",
      "--> Validation Accuracy Improved (0.0000 -> 0.0191). Saving model...\n",
      "\n",
      "Epoch 2/50\n",
      "----------\n",
      "Train Loss: 5.1057\n",
      "Val Loss:   5.0663\n",
      "Val Acc:    0.0293\n",
      "Current LR: 0.001\n",
      "--> Validation Accuracy Improved (0.0191 -> 0.0293). Saving model...\n",
      "\n",
      "Epoch 3/50\n",
      "----------\n",
      "Train Loss: 4.9573\n",
      "Val Loss:   6.6049\n",
      "Val Acc:    0.0127\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 4/50\n",
      "----------\n",
      "Train Loss: 4.8217\n",
      "Val Loss:   4.8753\n",
      "Val Acc:    0.0267\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 5/50\n",
      "----------\n",
      "Train Loss: 4.7318\n",
      "Val Loss:   4.8403\n",
      "Val Acc:    0.0483\n",
      "Current LR: 0.001\n",
      "--> Validation Accuracy Improved (0.0293 -> 0.0483). Saving model...\n",
      "\n",
      "Epoch 6/50\n",
      "----------\n",
      "Train Loss: 4.6374\n",
      "Val Loss:   4.7381\n",
      "Val Acc:    0.0445\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 7/50\n",
      "----------\n",
      "Train Loss: 4.5809\n",
      "Val Loss:   4.8679\n",
      "Val Acc:    0.0369\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 8/50\n",
      "----------\n",
      "Train Loss: 4.5080\n",
      "Val Loss:   4.8386\n",
      "Val Acc:    0.0496\n",
      "Current LR: 0.001\n",
      "--> Validation Accuracy Improved (0.0483 -> 0.0496). Saving model...\n",
      "\n",
      "Epoch 9/50\n",
      "----------\n",
      "Train Loss: 4.3665\n",
      "Val Loss:   4.7698\n",
      "Val Acc:    0.0560\n",
      "Current LR: 0.001\n",
      "--> Validation Accuracy Improved (0.0496 -> 0.0560). Saving model...\n",
      "\n",
      "Epoch 10/50\n",
      "----------\n",
      "Train Loss: 4.2916\n",
      "Val Loss:   4.6798\n",
      "Val Acc:    0.0687\n",
      "Current LR: 0.001\n",
      "--> Validation Accuracy Improved (0.0560 -> 0.0687). Saving model...\n",
      "\n",
      "Epoch 11/50\n",
      "----------\n",
      "Train Loss: 4.2253\n",
      "Val Loss:   4.4989\n",
      "Val Acc:    0.0712\n",
      "Current LR: 0.001\n",
      "--> Validation Accuracy Improved (0.0687 -> 0.0712). Saving model...\n",
      "\n",
      "Epoch 12/50\n",
      "----------\n",
      "Train Loss: 4.0906\n",
      "Val Loss:   4.5489\n",
      "Val Acc:    0.0776\n",
      "Current LR: 0.001\n",
      "--> Validation Accuracy Improved (0.0712 -> 0.0776). Saving model...\n",
      "\n",
      "Epoch 13/50\n",
      "----------\n",
      "Train Loss: 3.9719\n",
      "Val Loss:   4.6275\n",
      "Val Acc:    0.0852\n",
      "Current LR: 0.001\n",
      "--> Validation Accuracy Improved (0.0776 -> 0.0852). Saving model...\n",
      "\n",
      "Epoch 14/50\n",
      "----------\n",
      "Train Loss: 3.8956\n",
      "Val Loss:   4.4646\n",
      "Val Acc:    0.0738\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 15/50\n",
      "----------\n",
      "Train Loss: 3.7717\n",
      "Val Loss:   4.6457\n",
      "Val Acc:    0.0649\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 16/50\n",
      "----------\n",
      "Train Loss: 3.6745\n",
      "Val Loss:   4.3087\n",
      "Val Acc:    0.1069\n",
      "Current LR: 0.001\n",
      "--> Validation Accuracy Improved (0.0852 -> 0.1069). Saving model...\n",
      "\n",
      "Epoch 17/50\n",
      "----------\n",
      "Train Loss: 3.5944\n",
      "Val Loss:   4.2505\n",
      "Val Acc:    0.1158\n",
      "Current LR: 0.001\n",
      "--> Validation Accuracy Improved (0.1069 -> 0.1158). Saving model...\n",
      "\n",
      "Epoch 18/50\n",
      "----------\n",
      "Train Loss: 3.4211\n",
      "Val Loss:   4.1962\n",
      "Val Acc:    0.1043\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 19/50\n",
      "----------\n",
      "Train Loss: 3.3252\n",
      "Val Loss:   4.1898\n",
      "Val Acc:    0.1234\n",
      "Current LR: 0.001\n",
      "--> Validation Accuracy Improved (0.1158 -> 0.1234). Saving model...\n",
      "\n",
      "Epoch 20/50\n",
      "----------\n",
      "Train Loss: 3.2390\n",
      "Val Loss:   4.2916\n",
      "Val Acc:    0.1107\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 21/50\n",
      "----------\n",
      "Train Loss: 3.0481\n",
      "Val Loss:   4.1151\n",
      "Val Acc:    0.1310\n",
      "Current LR: 0.001\n",
      "--> Validation Accuracy Improved (0.1234 -> 0.1310). Saving model...\n",
      "\n",
      "Epoch 22/50\n",
      "----------\n",
      "Train Loss: 2.9367\n",
      "Val Loss:   4.0267\n",
      "Val Acc:    0.1196\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 23/50\n",
      "----------\n",
      "Train Loss: 2.7568\n",
      "Val Loss:   4.2374\n",
      "Val Acc:    0.1298\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 24/50\n",
      "----------\n",
      "Train Loss: 2.6686\n",
      "Val Loss:   4.0399\n",
      "Val Acc:    0.1387\n",
      "Current LR: 0.001\n",
      "--> Validation Accuracy Improved (0.1310 -> 0.1387). Saving model...\n",
      "\n",
      "Epoch 25/50\n",
      "----------\n",
      "Train Loss: 2.4896\n",
      "Val Loss:   4.0182\n",
      "Val Acc:    0.1374\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 26/50\n",
      "----------\n",
      "Train Loss: 2.2649\n",
      "Val Loss:   4.3157\n",
      "Val Acc:    0.1387\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 27/50\n",
      "----------\n",
      "Train Loss: 2.1675\n",
      "Val Loss:   4.2302\n",
      "Val Acc:    0.1132\n",
      "Current LR: 0.001\n",
      "\n",
      "Epoch 28/50\n",
      "----------\n",
      "Train Loss: 1.9612\n",
      "Val Loss:   4.1923\n",
      "Val Acc:    0.1438\n",
      "Current LR: 0.001\n",
      "--> Validation Accuracy Improved (0.1387 -> 0.1438). Saving model...\n",
      "\n",
      "Epoch 29/50\n",
      "----------\n",
      "Train Loss: 1.7273\n",
      "Val Loss:   4.5110\n",
      "Val Acc:    0.1221\n",
      "Current LR: 0.0001\n",
      "\n",
      "Epoch 30/50\n",
      "----------\n",
      "Train Loss: 1.2726\n",
      "Val Loss:   3.8434\n",
      "Val Acc:    0.1807\n",
      "Current LR: 0.0001\n",
      "--> Validation Accuracy Improved (0.1438 -> 0.1807). Saving model...\n",
      "\n",
      "Epoch 31/50\n",
      "----------\n",
      "Train Loss: 1.0264\n",
      "Val Loss:   3.8722\n",
      "Val Acc:    0.1794\n",
      "Current LR: 0.0001\n",
      "\n",
      "Epoch 32/50\n",
      "----------\n",
      "Train Loss: 0.9381\n",
      "Val Loss:   3.9249\n",
      "Val Acc:    0.1845\n",
      "Current LR: 0.0001\n",
      "--> Validation Accuracy Improved (0.1807 -> 0.1845). Saving model...\n",
      "\n",
      "Epoch 33/50\n",
      "----------\n",
      "Train Loss: 0.8959\n",
      "Val Loss:   3.9700\n",
      "Val Acc:    0.1718\n",
      "Current LR: 0.0001\n",
      "\n",
      "Epoch 34/50\n",
      "----------\n",
      "Train Loss: 0.8469\n",
      "Val Loss:   3.9472\n",
      "Val Acc:    0.1832\n",
      "Current LR: 1e-05\n",
      "\n",
      "Epoch 35/50\n",
      "----------\n",
      "Train Loss: 0.7728\n",
      "Val Loss:   3.9246\n",
      "Val Acc:    0.1807\n",
      "Current LR: 1e-05\n",
      "\n",
      "Epoch 36/50\n",
      "----------\n",
      "Train Loss: 0.7588\n",
      "Val Loss:   3.9567\n",
      "Val Acc:    0.1819\n",
      "Current LR: 1e-05\n",
      "\n",
      "Epoch 37/50\n",
      "----------\n",
      "Train Loss: 0.7497\n",
      "Val Loss:   3.9314\n",
      "Val Acc:    0.1845\n",
      "Current LR: 1e-05\n",
      "\n",
      "Epoch 38/50\n",
      "----------\n",
      "Train Loss: 0.7506\n",
      "Val Loss:   3.9377\n",
      "Val Acc:    0.1819\n",
      "Current LR: 1.0000000000000002e-06\n",
      "\n",
      "Epoch 39/50\n",
      "----------\n",
      "Train Loss: 0.7397\n",
      "Val Loss:   3.9433\n",
      "Val Acc:    0.1768\n",
      "Current LR: 1.0000000000000002e-06\n",
      "\n",
      "Epoch 40/50\n",
      "----------\n",
      "Train Loss: 0.7613\n",
      "Val Loss:   3.9411\n",
      "Val Acc:    0.1718\n",
      "Current LR: 1.0000000000000002e-06\n",
      "\n",
      "Epoch 41/50\n",
      "----------\n",
      "Train Loss: 0.7373\n",
      "Val Loss:   3.9810\n",
      "Val Acc:    0.1858\n",
      "Current LR: 1.0000000000000002e-06\n",
      "--> Validation Accuracy Improved (0.1845 -> 0.1858). Saving model...\n",
      "\n",
      "Epoch 42/50\n",
      "----------\n",
      "Train Loss: 0.7395\n",
      "Val Loss:   3.9325\n",
      "Val Acc:    0.1819\n",
      "Current LR: 1.0000000000000002e-07\n",
      "\n",
      "Epoch 43/50\n",
      "----------\n",
      "Train Loss: 0.7466\n",
      "Val Loss:   3.9603\n",
      "Val Acc:    0.1730\n",
      "Current LR: 1.0000000000000002e-07\n",
      "\n",
      "Epoch 44/50\n",
      "----------\n",
      "Train Loss: 0.7495\n",
      "Val Loss:   3.9378\n",
      "Val Acc:    0.1781\n",
      "Current LR: 1.0000000000000002e-07\n",
      "\n",
      "Epoch 45/50\n",
      "----------\n",
      "Train Loss: 0.7550\n",
      "Val Loss:   3.9413\n",
      "Val Acc:    0.1781\n",
      "Current LR: 1.0000000000000002e-07\n",
      "\n",
      "Epoch 46/50\n",
      "----------\n",
      "Train Loss: 0.7328\n",
      "Val Loss:   3.9428\n",
      "Val Acc:    0.1807\n",
      "Current LR: 1.0000000000000004e-08\n",
      "\n",
      "Epoch 47/50\n",
      "----------\n",
      "Train Loss: 0.7559\n",
      "Val Loss:   3.9719\n",
      "Val Acc:    0.1718\n",
      "Current LR: 1.0000000000000004e-08\n",
      "\n",
      "Epoch 48/50\n",
      "----------\n",
      "Train Loss: 0.7587\n",
      "Val Loss:   3.9294\n",
      "Val Acc:    0.1819\n",
      "Current LR: 1.0000000000000004e-08\n",
      "\n",
      "Epoch 49/50\n",
      "----------\n",
      "Train Loss: 0.7494\n",
      "Val Loss:   3.9570\n",
      "Val Acc:    0.1819\n",
      "Current LR: 1.0000000000000004e-08\n",
      "\n",
      "Epoch 50/50\n",
      "----------\n",
      "Train Loss: 0.7411\n",
      "Val Loss:   3.9239\n",
      "Val Acc:    0.1819\n",
      "Current LR: 1.0000000000000004e-08\n",
      "\n",
      "Training complete in 11m 38s\n",
      "Best Validation Accuracy: 0.1858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 6. TRAINING LOOP (With ResNet & Scheduler)\n",
    "# ==========================================\n",
    "import time\n",
    "\n",
    "# 1. SETUP\n",
    "# -----------------------------\n",
    "print(\"Initializing Custom ResNet...\")\n",
    "model = ResNetFromScratch(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# SCHEDULER: Checks 'val_loss' every epoch. \n",
    "# If it doesn't improve for 3 epochs ('patience'), it lowers LR by 10x ('factor').\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "# 2. TRAINING ENGINE\n",
    "# -----------------------------\n",
    "best_acc = 0.0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Starting training on {DEVICE} for {EPOCHS} epochs...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'\\nEpoch {epoch+1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    # --- TRAIN PHASE ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Train Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    # --- VALIDATION PHASE ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    # Calculate Metrics\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    val_acc = correct / total\n",
    "    \n",
    "    print(f\"Val Loss:   {val_loss:.4f}\")\n",
    "    print(f\"Val Acc:    {val_acc:.4f}\")\n",
    "    \n",
    "    # --- SCHEDULER STEP ---\n",
    "    # This is where the magic happens. We feed the validation loss to the scheduler.\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Print current Learning Rate (To verify scheduler is working)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Current LR: {current_lr}\")\n",
    "    \n",
    "    # --- SAVE BEST MODEL ---\n",
    "    if val_acc > best_acc:\n",
    "        print(f\"--> Validation Accuracy Improved ({best_acc:.4f} -> {val_acc:.4f}). Saving model...\")\n",
    "        best_acc = val_acc\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        torch.save(model.state_dict(), 'best_custom_model.pth')\n",
    "\n",
    "# End of training\n",
    "time_elapsed = time.time() - start_time\n",
    "print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "print(f'Best Validation Accuracy: {best_acc:.4f}')\n",
    "\n",
    "# Load best weights for final inference\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "546ec737",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T18:09:47.197427Z",
     "iopub.status.busy": "2025-12-13T18:09:47.197205Z",
     "iopub.status.idle": "2025-12-13T18:15:34.889523Z",
     "shell.execute_reply": "2025-12-13T18:15:34.888775Z"
    },
    "papermill": {
     "duration": 347.704566,
     "end_time": "2025-12-13T18:15:34.890626",
     "exception": false,
     "start_time": "2025-12-13T18:09:47.186060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ResNet with Dropout & OneCycleLR...\n",
      "Starting training on cuda for 25 epochs...\n",
      "Epoch 1/25 | Train Loss: 5.4069 | Val Loss: 5.1509 | Val Acc: 0.0229 | LR: 0.000245\n",
      "Epoch 2/25 | Train Loss: 5.1032 | Val Loss: 5.6120 | Val Acc: 0.0153 | LR: 0.000599\n",
      "Epoch 3/25 | Train Loss: 5.0328 | Val Loss: 6.4262 | Val Acc: 0.0165 | LR: 0.001120\n",
      "Epoch 4/25 | Train Loss: 4.9076 | Val Loss: 5.0520 | Val Acc: 0.0293 | LR: 0.001717\n",
      "Epoch 5/25 | Train Loss: 4.7700 | Val Loss: 5.0397 | Val Acc: 0.0369 | LR: 0.002287\n",
      "Epoch 6/25 | Train Loss: 4.7493 | Val Loss: 8.0671 | Val Acc: 0.0140 | LR: 0.002731\n",
      "Epoch 7/25 | Train Loss: 4.6103 | Val Loss: 5.1493 | Val Acc: 0.0433 | LR: 0.002971\n",
      "Epoch 8/25 | Train Loss: 4.4736 | Val Loss: 4.9540 | Val Acc: 0.0394 | LR: 0.002993\n",
      "Epoch 9/25 | Train Loss: 4.3446 | Val Loss: 4.7210 | Val Acc: 0.0547 | LR: 0.002945\n",
      "Epoch 10/25 | Train Loss: 4.1688 | Val Loss: 4.6990 | Val Acc: 0.0585 | LR: 0.002849\n",
      "Epoch 11/25 | Train Loss: 4.0666 | Val Loss: 4.8127 | Val Acc: 0.0547 | LR: 0.002710\n",
      "Epoch 12/25 | Train Loss: 3.9411 | Val Loss: 4.3668 | Val Acc: 0.0903 | LR: 0.002533\n",
      "Epoch 13/25 | Train Loss: 3.7515 | Val Loss: 4.2971 | Val Acc: 0.0865 | LR: 0.002322\n",
      "Epoch 14/25 | Train Loss: 3.5599 | Val Loss: 4.1286 | Val Acc: 0.1158 | LR: 0.002085\n",
      "Epoch 15/25 | Train Loss: 3.3317 | Val Loss: 3.8653 | Val Acc: 0.1221 | LR: 0.001829\n",
      "Epoch 16/25 | Train Loss: 3.1908 | Val Loss: 4.0138 | Val Acc: 0.1221 | LR: 0.001562\n",
      "Epoch 17/25 | Train Loss: 2.9561 | Val Loss: 3.8593 | Val Acc: 0.1450 | LR: 0.001293\n",
      "Epoch 18/25 | Train Loss: 2.7167 | Val Loss: 3.8159 | Val Acc: 0.1679 | LR: 0.001031\n",
      "Epoch 19/25 | Train Loss: 2.5576 | Val Loss: 3.7693 | Val Acc: 0.1463 | LR: 0.000784\n",
      "Epoch 20/25 | Train Loss: 2.2484 | Val Loss: 3.7631 | Val Acc: 0.1641 | LR: 0.000561\n",
      "Epoch 21/25 | Train Loss: 2.0065 | Val Loss: 3.6827 | Val Acc: 0.1718 | LR: 0.000367\n",
      "Epoch 22/25 | Train Loss: 1.8251 | Val Loss: 3.7940 | Val Acc: 0.1565 | LR: 0.000210\n",
      "Epoch 23/25 | Train Loss: 1.6294 | Val Loss: 3.6982 | Val Acc: 0.1781 | LR: 0.000094\n",
      "Epoch 24/25 | Train Loss: 1.5033 | Val Loss: 3.7396 | Val Acc: 0.1628 | LR: 0.000023\n",
      "Epoch 25/25 | Train Loss: 1.4490 | Val Loss: 3.7324 | Val Acc: 0.1667 | LR: 0.000000\n",
      "\n",
      "Training complete in 5m 47s\n",
      "Best Validation Accuracy: 0.1781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 6. SUPER-CONVERGENCE TRAINING LOOP\n",
    "# ==========================================\n",
    "import time\n",
    "\n",
    "# 1. SETUP\n",
    "# -----------------------------\n",
    "EPOCHS = 25 # Increased to give OneCycleLR room to work\n",
    "\n",
    "print(\"Initializing ResNet with Dropout & OneCycleLR...\")\n",
    "model = ResNetFromScratch(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# OPTIMIZER: AdamW (Better weight decay handling)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# SCHEDULER: OneCycleLR\n",
    "# This ramps the LR up to 'max_lr' then down to 0. \n",
    "# It is extremely effective for training from scratch.\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, \n",
    "    max_lr=0.003,              # Peak learning rate\n",
    "    epochs=EPOCHS, \n",
    "    steps_per_epoch=len(train_loader)\n",
    ")\n",
    "\n",
    "# 2. TRAINING ENGINE\n",
    "# -----------------------------\n",
    "best_acc = 0.0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Starting training on {DEVICE} for {EPOCHS} epochs...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    # --- TRAIN PHASE ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # STEP SCHEDULER (OneCycleLR updates every BATCH, not every EPOCH)\n",
    "        scheduler.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # --- VALIDATION PHASE ---\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels) # Calc val loss too\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    val_acc = correct / total\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    \n",
    "    # Get current LR for printout\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | LR: {current_lr:.6f}\")\n",
    "    \n",
    "    # --- SAVE BEST MODEL ---\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        torch.save(model.state_dict(), 'best_custom_model.pth')\n",
    "\n",
    "# End of training\n",
    "time_elapsed = time.time() - start_time\n",
    "print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "print(f'Best Validation Accuracy: {best_acc:.4f}')\n",
    "\n",
    "# Load best weights\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6edcfc4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T18:15:34.915481Z",
     "iopub.status.busy": "2025-12-13T18:15:34.915253Z",
     "iopub.status.idle": "2025-12-13T18:16:38.531435Z",
     "shell.execute_reply": "2025-12-13T18:16:38.530708Z"
    },
    "papermill": {
     "duration": 63.641835,
     "end_time": "2025-12-13T18:16:38.544290",
     "exception": false,
     "start_time": "2025-12-13T18:15:34.902455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n",
      "Saved submission_scratch.csv!\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 7. GENERATE SUBMISSION\n",
    "# ==========================================\n",
    "# Load Best Model\n",
    "model.load_state_dict(torch.load('best_custom_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "test_dataset = BirdDataset(\n",
    "    csv_file=f'{DATA_PATH}/test_images_path.csv',\n",
    "    root_dir=f'{DATA_PATH}',\n",
    "    transform=val_transforms # No augmentation for testing\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "all_preds = []\n",
    "all_ids = []\n",
    "\n",
    "print(\"Generating predictions...\")\n",
    "with torch.no_grad():\n",
    "    for inputs, ids in test_loader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        # Convert back to 1-200 range\n",
    "        predicted = predicted.cpu().numpy() + 1 \n",
    "        \n",
    "        all_preds.extend(predicted)\n",
    "        all_ids.extend(ids.numpy())\n",
    "\n",
    "# Save CSV\n",
    "submission = pd.DataFrame({'id': all_ids, 'label': all_preds})\n",
    "submission.to_csv('submission_scratch4.csv', index=False)\n",
    "print(\"Saved submission_scratch.csv!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13909210,
     "sourceId": 116520,
     "sourceType": "competition"
    },
    {
     "datasetId": 9010436,
     "sourceId": 14139279,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1821.605285,
   "end_time": "2025-12-13T18:16:40.178297",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-13T17:46:18.573012",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
