{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "07a2af8a",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "07a2af8a"
      },
      "source": [
        "# AML - 2025 : Feather in Focus - The Baseline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "uKSPK9FxZ-lB"
      },
      "id": "uKSPK9FxZ-lB",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. SETUP: Mount Google Drive\n",
        "# ---------------------------------------------------------\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "BASE_PATH = \"/content/drive/MyDrive/AML2025\"\n",
        "\n",
        "DATA_PATH = os.path.join(BASE_PATH, \"Dataset\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iweM-veyaIQ7",
        "outputId": "1329afc4-b1a4-4a83-c8a7-1a429a7907f3"
      },
      "id": "iweM-veyaIQ7",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. DEFINE THE DATASET CLASS\n",
        "# ---------------------------------------------------------\n",
        "class BirdDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # FIX: The CSV labels are 1-200, but PyTorch needs 0-199.\n",
        "        # We subtract 1 from every label.\n",
        "        self.data['label'] = self.data['label'] - 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get path from CSV (e.g., \"/train_images/1.jpg\")\n",
        "        img_path = self.data.iloc[idx, 0]\n",
        "\n",
        "        # Remove leading slash if present to join paths correctly\n",
        "        if img_path.startswith(\"/\"):\n",
        "            img_path = img_path[1:]\n",
        "\n",
        "        # Full path: /content/drive/.../train_images/1.jpg\n",
        "        full_path = os.path.join(self.root_dir, img_path)\n",
        "\n",
        "        # Load Image\n",
        "        try:\n",
        "            image = Image.open(full_path).convert(\"RGB\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"MISSING IMAGE: {full_path}\")\n",
        "            # Return a black image if file is missing (prevents crash)\n",
        "            image = Image.new('RGB', (224, 224))\n",
        "\n",
        "        label = self.data.iloc[idx, 1]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# 3. CREATE DATA LOADERS\n",
        "# ---------------------------------------------------------\n",
        "# Define standard formatting (Resize to 224x224)\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Initialize Dataset\n",
        "# Note: Pointing to where 'train_images.csv' is located\n",
        "dataset = BirdDataset(\n",
        "    csv_file=f'{DATA_PATH}/train_images.csv',\n",
        "    root_dir=DATA_PATH,\n",
        "    transform=data_transforms\n",
        ")\n",
        "\n",
        "# Split: 80% Train, 20% Validation\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create the Loaders (The final delivery)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\" Data Loaded Successfully!\")\n",
        "print(f\"Training Images: {len(train_dataset)}\")\n",
        "print(f\"Validation Images: {len(val_dataset)}\")"
      ],
      "metadata": {
        "id": "2R0uRJb16yhX"
      },
      "id": "2R0uRJb16yhX",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}